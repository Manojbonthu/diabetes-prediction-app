# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a_0q42SL0E30__KT7zW8idbkDyax4Fpo
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Machine Learning Libraries
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve

# For handling imbalanced data
from imblearn.over_sampling import SMOTE


# For saving model
import pickle
import joblib

# Load the Pima Indians Diabetes Dataset (commonly used for diabetes prediction)
# You can also use your own dataset
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
           'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']

# Load dataset
df = pd.read_csv(url, names=columns)

# Display basic information
print("Dataset Shape:", df.shape)
print("\nDataset Info:")
print(df.info())
print("\nFirst 5 rows:")
print(df.head())
print("\nStatistical Summary:")
print(df.describe())
print("\nTarget Distribution:")
print(df['Outcome'].value_counts())

class DiabetesDataPreprocessor:
    def __init__(self, df):
        self.df = df.copy()

    def handle_missing_values(self):
        """Replace zeros with NaN for columns where 0 is not valid"""
        columns_with_zeros = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

        for col in columns_with_zeros:
            self.df[col] = self.df[col].replace(0, np.nan)

        # Fill missing values with median
        for col in columns_with_zeros:
            median_val = self.df[col].median()
            self.df[col].fillna(median_val, inplace=True)

        return self

    def remove_outliers(self):
        """Remove outliers using IQR method"""
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        numeric_cols = numeric_cols.drop('Outcome')  # Exclude target variable

        for col in numeric_cols:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1

            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            self.df = self.df[(self.df[col] >= lower_bound) & (self.df[col] <= upper_bound)]

        return self

    def feature_engineering(self):
        """Create new features"""
        # BMI categories
        self.df['BMI_Category'] = pd.cut(self.df['BMI'],
                                         bins=[0, 18.5, 25, 30, 100],
                                         labels=['Underweight', 'Normal', 'Overweight', 'Obese'])

        # Age categories
        self.df['Age_Category'] = pd.cut(self.df['Age'],
                                         bins=[0, 30, 40, 50, 100],
                                         labels=['Young', 'Middle', 'Senior', 'Elderly'])

        # Convert categorical to numeric
        self.df = pd.get_dummies(self.df, columns=['BMI_Category', 'Age_Category'], drop_first=True)

        return self

    def get_processed_data(self):
        return self.df

# Apply preprocessing
preprocessor = DiabetesDataPreprocessor(df)
df_processed = (preprocessor
                .handle_missing_values()
                .remove_outliers()
                .feature_engineering()
                .get_processed_data())

print("Shape after preprocessing:", df_processed.shape)

# Separate features and target
X = df_processed.drop('Outcome', axis=1)
y = df_processed['Outcome']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"\nClass distribution:")
print(y.value_counts())

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print(f"Original dataset shape: {X.shape}")
print(f"Resampled dataset shape: {X_resampled.shape}")
print(f"\nResampled class distribution:")
print(pd.Series(y_resampled).value_counts())

# 80-20 train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training set shape: {X_train_scaled.shape}")
print(f"Test set shape: {X_test_scaled.shape}")

# SVM with hyperparameter tuning
svm_params = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01],
    'kernel': ['rbf', 'poly', 'sigmoid']
}

svm_model = SVC(probability=True, random_state=42)
svm_grid = GridSearchCV(svm_model, svm_params, cv=5, scoring='accuracy', n_jobs=-1)
svm_grid.fit(X_train_scaled, y_train)

best_svm = svm_grid.best_estimator_
print("Best SVM parameters:", svm_grid.best_params_)
print("Best SVM CV score:", svm_grid.best_score_)

# Random Forest with hyperparameter tuning
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_model = RandomForestClassifier(random_state=42)
rf_grid = GridSearchCV(rf_model, rf_params, cv=5, scoring='accuracy', n_jobs=-1)
rf_grid.fit(X_train_scaled, y_train)

best_rf = rf_grid.best_estimator_
print("Best RF parameters:", rf_grid.best_params_)
print("Best RF CV score:", rf_grid.best_score_)

# KNN with hyperparameter tuning
knn_params = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

knn_model = KNeighborsClassifier()
knn_grid = GridSearchCV(knn_model, knn_params, cv=5, scoring='accuracy', n_jobs=-1)
knn_grid.fit(X_train_scaled, y_train)

best_knn = knn_grid.best_estimator_
print("Best KNN parameters:", knn_grid.best_params_)
print("Best KNN CV score:", knn_grid.best_score_)

# Create voting classifier with optimized models
voting_classifier = VotingClassifier(
    estimators=[
        ('svm', best_svm),
        ('rf', best_rf),
        ('knn', best_knn)
    ],
    voting='soft'  # Use soft voting for probability-based predictions
)

# Train the voting classifier
voting_classifier.fit(X_train_scaled, y_train)

# Make predictions
y_pred = voting_classifier.predict(X_test_scaled)
y_pred_proba = voting_classifier.predict_proba(X_test_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nVoting Classifier Accuracy: {accuracy:.4f}")
print(f"Accuracy Percentage: {accuracy * 100:.2f}%")



import skfuzzy as fuzz
from skfuzzy import control as ctrl

def evaluate_model(y_true, y_pred, y_pred_proba, model_name="Model"):
    """Comprehensive model evaluation"""
    print(f"\n{'='*50}")
    print(f"{model_name} Evaluation Results")
    print('='*50)

    # Accuracy
    accuracy = accuracy_score(y_true, y_pred)
    print(f"Accuracy: {accuracy:.4f}")

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['Non-Diabetic', 'Diabetic']))

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    print("\nConfusion Matrix:")
    print(cm)

    # ROC AUC Score
    roc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])
    print(f"\nROC AUC Score: {roc_auc:.4f}")

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model_name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    return accuracy

# Evaluate the voting classifier
evaluate_model(y_test, y_pred, y_pred_proba, "Voting Classifier")

class FuzzyLogicHealthRecommendation:
    def __init__(self):
        self.setup_fuzzy_system()

    def setup_fuzzy_system(self):
        """Setup fuzzy logic system for health recommendations"""
        # Define fuzzy variables
        self.confidence = ctrl.Antecedent(np.arange(0, 1.01, 0.01), 'confidence')
        self.glucose = ctrl.Antecedent(np.arange(0, 200, 1), 'glucose')
        self.bmi = ctrl.Antecedent(np.arange(0, 50, 0.1), 'bmi')
        self.risk_level = ctrl.Consequent(np.arange(0, 101, 1), 'risk_level')

        # Define membership functions
        self.confidence['low'] = fuzz.trimf(self.confidence.universe, [0, 0, 0.5])
        self.confidence['medium'] = fuzz.trimf(self.confidence.universe, [0.3, 0.5, 0.7])
        self.confidence['high'] = fuzz.trimf(self.confidence.universe, [0.5, 1, 1])

        self.glucose['low'] = fuzz.trimf(self.glucose.universe, [0, 0, 100])
        self.glucose['normal'] = fuzz.trimf(self.glucose.universe, [80, 110, 140])
        self.glucose['high'] = fuzz.trimf(self.glucose.universe, [120, 200, 200])

        self.bmi['normal'] = fuzz.trimf(self.bmi.universe, [0, 18.5, 25])
        self.bmi['overweight'] = fuzz.trimf(self.bmi.universe, [23, 27.5, 32])
        self.bmi['obese'] = fuzz.trimf(self.bmi.universe, [30, 50, 50])

        self.risk_level['low'] = fuzz.trimf(self.risk_level.universe, [0, 0, 40])
        self.risk_level['medium'] = fuzz.trimf(self.risk_level.universe, [30, 50, 70])
        self.risk_level['high'] = fuzz.trimf(self.risk_level.universe, [60, 100, 100])

        # Define fuzzy rules
        rule1 = ctrl.Rule(self.confidence['high'] & self.glucose['high'] & self.bmi['obese'],
                         self.risk_level['high'])
        rule2 = ctrl.Rule(self.confidence['medium'] & self.glucose['normal'] & self.bmi['normal'],
                         self.risk_level['low'])
        rule3 = ctrl.Rule(self.confidence['high'] & self.glucose['normal'] & self.bmi['overweight'],
                         self.risk_level['medium'])
        rule4 = ctrl.Rule(self.confidence['low'] | self.glucose['low'],
                         self.risk_level['low'])
        rule5 = ctrl.Rule(self.glucose['high'] & self.bmi['overweight'],
                         self.risk_level['high'])

        # Create control system
        self.risk_ctrl = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5])
        self.risk_simulation = ctrl.ControlSystemSimulation(self.risk_ctrl)

    def get_recommendation(self, confidence_score, glucose_level, bmi_value):
        """Get personalized health recommendation"""
        try:
            self.risk_simulation.input['confidence'] = confidence_score
            self.risk_simulation.input['glucose'] = glucose_level
            self.risk_simulation.input['bmi'] = bmi_value

            self.risk_simulation.compute()
            risk_score = self.risk_simulation.output['risk_level']

            # Generate recommendation based on risk score
            if risk_score < 30:
                return {
                    'risk_level': 'Low',
                    'recommendation': 'Maintain healthy lifestyle. Continue regular exercise and balanced diet.',
                    'risk_score': risk_score
                }
            elif risk_score < 70:
                return {
                    'risk_level': 'Medium',
                    'recommendation': 'Monitor blood sugar regularly. Consider dietary improvements and increase physical activity.',
                    'risk_score': risk_score
                }
            else:
                return {
                    'risk_level': 'High',
                    'recommendation': 'Consult healthcare provider immediately. Strict diet control and medication may be needed.',
                    'risk_score': risk_score
                }
        except:
            return {
                'risk_level': 'Unknown',
                'recommendation': 'Please provide valid health metrics for assessment.',
                'risk_score': 0
            }

# Initialize fuzzy logic system
fuzzy_system = FuzzyLogicHealthRecommendation()

class DiabetesPredictionPipeline:
    def __init__(self, model, scaler, fuzzy_system):
        self.model = model
        self.scaler = scaler
        self.fuzzy_system = fuzzy_system

    def predict_diabetes(self, input_data):
        """Complete prediction with fuzzy logic recommendations"""
        # Scale input data
        input_scaled = self.scaler.transform(input_data)

        # Get prediction and probability
        prediction = self.model.predict(input_scaled)[0]
        probability = self.model.predict_proba(input_scaled)[0]

        # Get confidence score
        confidence = max(probability)

        # Extract glucose and BMI from input (adjust indices based on your features)
        glucose = input_data[0][1]  # Assuming glucose is at index 1
        bmi = input_data[0][5]  # Assuming BMI is at index 5

        # Get fuzzy logic recommendation
        recommendation = self.fuzzy_system.get_recommendation(confidence, glucose, bmi)

        result = {
            'prediction': 'Diabetic' if prediction == 1 else 'Non-Diabetic',
            'probability_diabetic': probability[1],
            'probability_non_diabetic': probability[0],
            'confidence': confidence,
            'risk_assessment': recommendation
        }

        return result

# Create pipeline instance
pipeline = DiabetesPredictionPipeline(voting_classifier, scaler, fuzzy_system)

# Test with a sample
sample_data = X_test.iloc[0:1].values
result = pipeline.predict_diabetes(sample_data)

print("\nSample Prediction Result:")
print(f"Prediction: {result['prediction']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Risk Level: {result['risk_assessment']['risk_level']}")
print(f"Recommendation: {result['risk_assessment']['recommendation']}")

# Create a dictionary containing all model components
model_package = {
    'voting_classifier': voting_classifier,
    'scaler': scaler,
    'svm_model': best_svm,
    'rf_model': best_rf,
    'knn_model': best_knn,
    'feature_columns': X_train.columns.tolist(),
    'model_performance': {
        'accuracy': accuracy,
        'test_size': len(X_test),
        'train_size': len(X_train)
    }
}

# ==============================
# Ensure models/ folder exists
# ==============================
import os
MODELS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'diabetes_app', 'models')
os.makedirs(MODELS_DIR, exist_ok=True)

# Save using pickle (inside models/)
pkl_path = os.path.join(MODELS_DIR, 'diabetes_prediction_model.pkl')
with open(pkl_path, 'wb') as file:
    pickle.dump(model_package, file)

print(f"Model saved successfully as '{pkl_path}'")

# Save individual components using joblib (inside models/)
joblib.dump(voting_classifier, os.path.join(MODELS_DIR, 'voting_classifier.joblib'))
joblib.dump(scaler,            os.path.join(MODELS_DIR, 'scaler.joblib'))
joblib.dump(best_svm,          os.path.join(MODELS_DIR, 'svm_model.joblib'))
joblib.dump(best_rf,           os.path.join(MODELS_DIR, 'rf_model.joblib'))
joblib.dump(best_knn,          os.path.join(MODELS_DIR, 'knn_model.joblib'))

print("Individual model components saved using joblib inside models/")

# Load the complete model package
with open(pkl_path, 'rb') as file:
    loaded_model_package = pickle.load(file)

# Extract components
loaded_classifier = loaded_model_package['voting_classifier']
loaded_scaler = loaded_model_package['scaler']
loaded_feature_columns = loaded_model_package['feature_columns']

# Make predictions with loaded model
def predict_with_loaded_model(features):
    """
    features: list or array with patient data
    Order: [Pregnancies, Glucose, BloodPressure, SkinThickness,
            Insulin, BMI, DiabetesPedigreeFunction, Age, ...]
    """
    features_df = pd.DataFrame([features], columns=loaded_model_package['feature_columns'])
    scaled_features = loaded_scaler.transform(features_df)

    prediction = loaded_classifier.predict(scaled_features)[0]
    probability = loaded_classifier.predict_proba(scaled_features)[0]

    return {
        'prediction': 'Diabetic' if prediction == 1 else 'Non-Diabetic',
        'diabetic_probability': f"{probability[1]:.2%}",
        'non_diabetic_probability': f"{probability[0]:.2%}"
    }

# Example usage
print("\nLoaded Model Test:")
test_patient = X_test.iloc[5].values
result = predict_with_loaded_model(test_patient)
print(f"Prediction: {result['prediction']}")
print(f"Diabetic Probability: {result['diabetic_probability']}")

def generate_performance_report():
    """Generate comprehensive performance report"""
    print("\n" + "="*60)
    print("DIABETES PREDICTION MODEL - PERFORMANCE REPORT")
    print("="*60)

    # Individual model performances
    models = {
        'SVM': best_svm,
        'Random Forest': best_rf,
        'KNN': best_knn,
        'Voting Classifier': voting_classifier
    }

    results = {}
    for name, model in models.items():
        y_pred = model.predict(X_test_scaled)
        acc = accuracy_score(y_test, y_pred)
        results[name] = acc
        print(f"{name}: {acc:.4f} ({acc*100:.2f}%)")

    print("\n" + "-"*60)
    print(f"Best Individual Model: {max(results, key=results.get)}")
    print(f"Fused Model (Voting Classifier) Accuracy: {results['Voting Classifier']:.4f}")
    print(f"Improvement over best individual: " +
          f"{(results['Voting Classifier'] - max([v for k,v in results.items() if k != 'Voting Classifier']))*100:.2f}%")

    # Cross-validation scores
    print("\n" + "-"*60)
    print("Cross-Validation Scores (5-fold):")
    cv_scores = cross_val_score(voting_classifier, X_train_scaled, y_train, cv=5, scoring='accuracy')
    print(f"Mean CV Score: {cv_scores.mean():.4f}")
    print(f"Standard Deviation: {cv_scores.std():.4f}")

    return results

# Generate report
performance_results = generate_performance_report()